{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Utkarsha88/ANAIS_2025/blob/main/anais_gdl_p1_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c431a61",
      "metadata": {
        "id": "4c431a61"
      },
      "source": [
        "Authors: A. Lupidi (alisia.lupidi@cs.ox.ac.uk), T. Reu (teodora.reu@cs.ox.ac.uk)\n",
        "\n",
        "# ANAIS Geometric Deep Learning\n",
        "## Practical 1: GNNs\n",
        "\n",
        "*Welcome to our first practical* ğŸš€ \\\n",
        "This notebook will focus on Graph Neural Networks.\n",
        "In the following sections, we will:\n",
        "- Part 0: a quick refresh on theory and set up\n",
        "- Part 1: building a GNN\n",
        "- Part 2: exploring expressivity of GNNs\n",
        "- Part 3: exploring equivarance and invariance\n",
        "\n",
        "This content is adapted from the Geometric Deep Learning course at the University of Cambridge (credits P. Lio, P. VeliÄkoviÄ‡, C. K. Joshi, C. Harris, R. TornÃ©, C. Bodnar) and the Graph Representation Learning course at the University of Oxford (credits Ä°. Ceylan, F. Barbero, X. Huang).\n",
        "\n",
        "References:\n",
        "- The Geometric Deep Learning Book (M. M. Bronstein, J. Bruna, T. Cohen, P. VeliÄkoviÄ‡ 2021)\n",
        "\n",
        "# **Your tasks**\n",
        "The aim of this tutorial is to provide you with a practical understanding of the theory and coding required in Parts 1-3.\n",
        "Practically, you will need to complete the GIN Encoder and Decoder classes in `Message Passing GNN (GIN) + MLP decoder` and answer the question in section `1. Exercise`.\n",
        "Throughout the tutorial, we have dispensed nuggets of theory, demostrated in code. Try tackling the coding exercise on your own or in groups, don't worry if you don't finish it. Also, do spend some time and thinking on the definitions and readings, we will have an interactive discussion at the end. Teo and Alisia will demonstrate will demonstrate the solution to the exercise at the end of the turorial, don't hesitate to ask any questions!\n",
        "\n",
        "We hope you enjoy the exercise and the reading.\n",
        "Happy GNNsğŸš€\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41edba15",
      "metadata": {
        "id": "41edba15"
      },
      "source": [
        "# ğŸ“– Part 0.a: a refresh on theory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iFbpw4kPro8l",
      "metadata": {
        "id": "iFbpw4kPro8l"
      },
      "source": [
        "<img src=\"https://github.com/metalisia/gdl_images/blob/main/gldp1f1.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e79baeb9",
      "metadata": {
        "id": "e79baeb9"
      },
      "source": [
        "#### What is a Graph?\n",
        "\n",
        "A graph is a tuple $\\mathbb{G}=(\\mathbb{V}, \\mathbb{E})$  where $\\mathbb{V}$ is a set of nodes (or vertices) and $\\mathbb{E} âŠ† (\\mathbb{V} Ã— \\mathbb{V})$ is a 2-tuple set of the edges in the graph. The element $(\\mathbb{u}, \\mathbb{v}) âˆˆ \\mathbb{E}$ is a directed from $\\mathbb{u}$ to $\\mathbb{v}$ and indicates a relationship between nodes $\\mathbb{u}$ and $\\mathbb{v}$.\n",
        "\n",
        "The most convenient way to represent a graph is by adjeciency matrix. Given a graph $\\mathbb{G}$ defined as above where $|\\mathbb{V}| = n$ nodes, $A \\in \\mathbb{R}^{n \\times n}$ is a symmetric adjacency matrix where is $a_{i,j}$ is the weight of the edge between nodes $v_i$ and $v_j$. If $(v_i, v_j) \\notin E$ then $a_{i,j} = 0$.\n",
        "\n",
        "Graphs can store layers of information in the nodes and edges (features). For graphs with node features, each node $v_i \\in \\mathbb{V}$ has an associated $d$-dimensional feature vector $\\mathbf{x_i} \\in \\mathbb{R}^{d}$. Then the feature matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ can be used to represent the feature vectors for every node in the graph.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5db2ef",
      "metadata": {
        "id": "9b5db2ef"
      },
      "source": [
        "#### What is a Graph Neural Network?\n",
        "A GNN is a Neural Network architecture built for handling structured data in the shape of graphs. GNNs update node representations by aggregating information from neighboring nodes and edges (message passing). After several rounds of aggregation, each nodeâ€™s representation encodes information about its local graph structure. This makes them powerful tools for tasks like node / edge / graph-level prediction or classification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea370ee9",
      "metadata": {
        "id": "ea370ee9"
      },
      "source": [
        "# âš™ï¸ Part 0.b: set up"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a780c707",
      "metadata": {
        "id": "a780c707"
      },
      "source": [
        "The code below is setting up the environment for this practical along with defining some handy functions for reproducibility,\n",
        "\n",
        "**â—ï¸Note:** You will need a GPU to complete this practical. Remember to click `Runtime -> Change runtime type`, and set the `hardware accelerator` to **GPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8ccf4e68",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8ccf4e68",
        "outputId": "e1f71945-e8a0-4041-a908-c97fa04424be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==2.2.0+cu118\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.2.0%2Bcu118-cp312-cp312-linux_x86_64.whl (811.6 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (4.15.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.2.0+cu118) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu11==11.8.89 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu11==11.8.87 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu11==8.7.0.84 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu11==11.11.3.6 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu11==10.9.0.58 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu11==10.3.0.86 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu11==11.4.1.48 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu11==11.7.5.86 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-nccl-cu11==2.19.3 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu11==11.8.86 (from torch==2.2.0+cu118)\n",
            "  Using cached nvidia_nvtx_cu11-11.8.86-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.2.0+cu118) (3.0.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->torch==2.2.0+cu118) (1.3.0)\n",
            "Using cached nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux2014_x86_64.whl (417.9 MB)\n",
            "Using cached nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux2014_x86_64.whl (13.1 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (23.2 MB)\n",
            "Using cached nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux2014_x86_64.whl (875 kB)\n",
            "Downloading nvidia_cudnn_cu11-8.7.0.84-py3-none-manylinux1_x86_64.whl (728.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m728.5/728.5 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m\n",
            "\u001b[?25hUsing cached nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux2014_x86_64.whl (168.4 MB)\n",
            "Using cached nvidia_curand_cu11-10.3.0.86-py3-none-manylinux2014_x86_64.whl (58.1 MB)\n",
            "Using cached nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux2014_x86_64.whl (128.2 MB)\n",
            "Using cached nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux2014_x86_64.whl (204.1 MB)\n",
            "Using cached nvidia_nccl_cu11-2.19.3-py3-none-manylinux1_x86_64.whl (135.3 MB)\n",
            "Using cached nvidia_nvtx_cu11-11.8.86-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
            "Installing collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.9.0+cu126\n",
            "\u001b[2K    Uninstalling torch-2.9.0+cu126:\n",
            "\u001b[2K      Successfully uninstalled torch-2.9.0+cu126\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/12\u001b[0m [torch]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.2.0+cu118 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.2.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-8.7.0.84 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.19.3 nvidia-nvtx-cu11-11.8.86 torch-2.2.0+cu118\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Collecting torch==2.9.0 (from torchvision)\n",
            "  Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (80.9.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.9.0->torchvision)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
            "Downloading torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.7/899.7 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m  \u001b[33m0:00:15\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m  \u001b[33m0:00:16\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m163.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m70.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cusolver-cu12, torch\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufile-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "\u001b[2K    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.2.0+cu118\n",
            "\u001b[2K    Uninstalling torch-2.2.0+cu118:\n",
            "\u001b[2K      Successfully uninstalled torch-2.2.0+cu118\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12/12\u001b[0m [torch]\n",
            "\u001b[1A\u001b[2KSuccessfully installed nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 torch-2.9.0\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.2.0+cu118 -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install torchvision torchaudio -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install networkx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "nYLwX_ltIVM_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYLwX_ltIVM_",
        "outputId": "a25a7cfe-804e-4cd7-d095-3ea70b55ff22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.12/dist-packages (0.45.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (80.9.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U pip wheel setuptools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "VgZ4DqkhIXr1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgZ4DqkhIXr1",
        "outputId": "77e1bdbf-c9b8-4990-fb0b-35e4f816b3f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_scatter-2.1.2%2Bpt25cu121-cp312-cp312-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_sparse-0.6.18%2Bpt25cu121-cp312-cp312-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_cluster-1.6.3%2Bpt25cu121-cp312-cp312-linux_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch-spline-conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.5.0%2Bcu121/torch_spline_conv-1.2.2%2Bpt25cu121-cp312-cp312-linux_x86_64.whl (994 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m994.8/994.8 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-spline-conv, torch-scatter, torch-sparse, torch-cluster\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4/4\u001b[0m [torch-cluster]\n",
            "\u001b[1A\u001b[2KSuccessfully installed torch-cluster-1.6.3+pt25cu121 torch-scatter-2.1.2+pt25cu121 torch-sparse-0.6.18+pt25cu121 torch-spline-conv-1.2.2+pt25cu121\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.5.1+cu121.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "l0blAxmTKvXw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0blAxmTKvXw",
        "outputId": "5b2e0081-110e-4f10-8c65-4dcd589c7df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.5.1+cu121\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp312-cp312-linux_x86_64.whl (780.4 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m780.4/780.4 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m183.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m188.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m  \u001b[33m0:00:09\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m64.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m71.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m62.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m  \u001b[33m0:00:06\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.21.5 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m  \u001b[33m0:00:12\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1+cu121)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.6 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.6/209.6 MB\u001b[0m \u001b[31m?\u001b[0m  \u001b[33m0:00:48\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.5.1+cu121) (80.9.0)\n",
            "Collecting sympy==1.13.1 (from torch==2.5.1+cu121)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.12/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.5.1+cu121) (12.8.93)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch==2.5.1+cu121) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.5.1+cu121) (3.0.3)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, sympy, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.5.0\n",
            "\u001b[2K    Uninstalling triton-3.5.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.5.0\n",
            "\u001b[2K  Attempting uninstall: sympy\n",
            "\u001b[2K    Found existing installation: sympy 1.14.0\n",
            "\u001b[2K    Uninstalling sympy-1.14.0:\n",
            "\u001b[2K      Successfully uninstalled sympy-1.14.0\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.8.90\n",
            "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.8.90:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.8.90\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.27.5\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.27.5:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.27.5\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.8.93\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.8.93:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.8.93\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.9.90\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.9.90:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.9.90\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.3.3.83\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.3.3.83:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.3.3.83\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.8.90\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.8.90:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.8.90\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.8.93\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.8.93:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.8.93\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.8.90\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.8.90:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.8.90\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.8.4.1\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.8.4.1:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.8.4.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.7.3.90\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.7.3.90:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.7.3.90\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.10.2.21\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.10.2.21:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.10.2.21\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.9.0\n",
            "\u001b[2K    Uninstalling torch-2.9.0:\n",
            "\u001b[2K      Successfully uninstalled torch-2.9.0\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14/14\u001b[0m [torch]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.9.0+cu126 requires torch==2.9.0, but you have torch 2.5.1+cu121 which is incompatible.\n",
            "torchvision 0.24.0+cu126 requires torch==2.9.0, but you have torch 2.5.1+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.21.5 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.5.1+cu121 triton-3.1.0\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt25cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt25cu121)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.12/dist-packages (1.6.3+pt25cu121)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.12/dist-packages (1.2.2+pt25cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp->torch-geometric) (3.11)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.11.12)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --index-url https://download.pytorch.org/whl/cu121 torch==2.5.1+cu121\n",
        "!python -m pip install torch-scatter torch-sparse torch-cluster torch-spline-conv -f https://data.pyg.org/whl/torch-2.5.1+cu121.html\n",
        "!python -m pip install torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7c5acf6c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c5acf6c",
        "outputId": "cba92f47-412d-473d-e8db-68e783de2727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version 2.5.1+cu121\n",
            "PyG version 2.7.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from scipy.stats import ortho_group\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, ReLU, BatchNorm1d, Module, Sequential\n",
        "\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.datasets import QM9\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.utils import remove_self_loops, to_dense_adj, dense_to_sparse\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import MessagePassing, global_mean_pool\n",
        "from torch_scatter import scatter\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import files\n",
        "from IPython.display import HTML\n",
        "\n",
        "print(\"PyTorch version {}\".format(torch.__version__))\n",
        "print(\"PyG version {}\".format(torch_geometric.__version__))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ddb1d55f",
      "metadata": {
        "id": "ddb1d55f"
      },
      "outputs": [],
      "source": [
        "def seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f73156b",
      "metadata": {
        "id": "8f73156b"
      },
      "source": [
        "Great! We are ready to dive into the practical!\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03dc1079",
      "metadata": {
        "id": "03dc1079"
      },
      "source": [
        "#ğŸ¥‡ Part 1: How to build a (Message Passing) Graph Neural Network\n",
        "\n",
        "\n",
        "<!-- ![](https://drive.google.com/uc?id=1Wdgdq606XW1MelvcU1nW5CxWe1rHsWt1) -->\n",
        "<img src=\"https://github.com/chaitjo/dump/raw/main/gnn-layers.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XPv9XuhhP0cr",
      "metadata": {
        "id": "XPv9XuhhP0cr"
      },
      "source": [
        "## Message Passing Neural Network for Link Prediction on Cora\n",
        "\n",
        "**Goal.** Given the Cora citation graph $\\mathbb{G}=(\\mathbb{V},\\mathbb{E})$, learn a model that predicts whether a link exists between a pair of nodes $(\\mathbb{u},\\mathbb{v})$. This is a **binary classification** task over candidate node pairs.\n",
        "\n",
        "Cora provides node features $x_i \\in \\mathbb{R}^{d_n}$ for each node $i\\in V$. It typically has **no explicit edge features**; therefore we either:\n",
        "- omit edge features from the message function, or\n",
        "- define constant edge features $e_{ij}=\\mathbf{0}\\in\\mathbb{R}^{d_e}$ (optional).\n",
        "\n",
        "---\n",
        "\n",
        "### Encoder: $L$-Layer Message Passing Network\n",
        "\n",
        "We compute hidden node representations $h_i^\\ell \\in \\mathbb{R}^{d}$ for $\\ell=0,\\dots,L$.\n",
        "\n",
        "#### Input Projection\n",
        "Project input node features to the hidden dimension:\n",
        "$$\n",
        "h_i^{0} = W_{\\text{in}} x_i,\n",
        "\\qquad W_{\\text{in}} \\in \\mathbb{R}^{d \\times d_n}.\n",
        "$$\n",
        "\n",
        "#### MPNN Layer\n",
        "For each layer $\\ell \\to \\ell+1$, update node embeddings via message passing:\n",
        "$$\n",
        "h_i^{\\ell+1} = \\phi\\Bigg(\n",
        "h_i^{\\ell},\n",
        "\\ \\ \\oplus_{j\\in\\mathcal{N}(i)} \\psi\\left(h_i^{\\ell}, h_j^{\\ell}, e_{ij}\\right)\n",
        "\\Bigg),\n",
        "$$\n",
        "where:\n",
        "- $\\mathcal{N}(i)$ is the neighbor set of node $i$,\n",
        "- $\\oplus$ is a permutation-invariant aggregator (we use summation),\n",
        "- $\\psi$ is a message MLP,\n",
        "- $\\phi$ is an update MLP.\n",
        "\n",
        "Pedagogical decomposition:\n",
        "\n",
        "1) **Message**\n",
        "$$\n",
        "m_{ij}^{\\ell}=\\psi\\left(h_i^\\ell, h_j^\\ell, e_{ij}\\right),\n",
        "\\qquad \\psi:\\mathbb{R}^{2d+d_e}\\to\\mathbb{R}^{d}.\n",
        "$$\n",
        "\n",
        "2) **Aggregate**\n",
        "$$\n",
        "m_i^\\ell = \\sum_{j \\in \\mathcal{N}(i)} m_{ij}^{\\ell}.\n",
        "$$\n",
        "\n",
        "3) **Update**\n",
        "$$\n",
        "h_i^{\\ell+1} = \\phi\\left(\\left[h_i^\\ell \\ \\| \\ m_i^\\ell\\right]\\right),\n",
        "\\qquad \\phi:\\mathbb{R}^{2d}\\to\\mathbb{R}^{d},\n",
        "$$\n",
        "where $\\cdot \\| \\cdot$ denotes concatenation.\n",
        "\n",
        "After $L$ layers, define the final node embedding:\n",
        "$$\n",
        "z_i := h_i^{L}\\in\\mathbb{R}^{d}.\n",
        "$$\n",
        "\n",
        "**Note.** For link prediction we do **not** apply global pooling; we require node-level embeddings $z_i$ to score specific pairs $(u,v)$.\n",
        "\n",
        "---\n",
        "\n",
        "### Decoder: Edge Scoring / Prediction Head\n",
        "\n",
        "For a candidate node pair $(u,v)$, compute a scalar **logit** $s_{uv}\\in\\mathbb{R}$ and probability $\\hat p_{uv}\\in(0,1)$.\n",
        "\n",
        "A common MLP-based decoder uses pairwise features:\n",
        "$$\n",
        "s_{uv} = g\\left(\\left[z_u \\ \\| \\ z_v \\ \\| \\ (z_u\\odot z_v)\\right]\\right),\n",
        "\\qquad g:\\mathbb{R}^{3d}\\to\\mathbb{R},\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat p_{uv} = \\sigma(s_{uv}),\n",
        "$$\n",
        "where $\\odot$ is elementwise product and $\\sigma(\\cdot)$ is the sigmoid.\n",
        "\n",
        "(Alternative decoders include dot-product $s_{uv}=z_u^\\top z_v$ or bilinear scoring.)\n",
        "\n",
        "---\n",
        "\n",
        "### Link Prediction Splits (Train / Val / Test)\n",
        "\n",
        "We create disjoint sets of **positive** edges:\n",
        "$$\n",
        "E^+_{\\text{train}},\\ E^+_{\\text{val}},\\ E^+_{\\text{test}}\n",
        "\\quad \\text{with} \\quad\n",
        "E^+_{\\text{train}} \\cup E^+_{\\text{val}} \\cup E^+_{\\text{test}} \\subseteq E.\n",
        "$$\n",
        "\n",
        "During training, message passing is performed on a **training graph**\n",
        "$$\n",
        "G_{\\text{mp}} = (V, E_{\\text{mp}}),\n",
        "$$\n",
        "where $E_{\\text{mp}}$ excludes held-out validation/test positives to prevent leakage.\n",
        "\n",
        "For supervised learning and evaluation, we also require **negative** edges:\n",
        "$$\n",
        "E^-_{\\text{train}} \\subset (V\\times V)\\setminus E,\n",
        "\\quad\n",
        "E^-_{\\text{val}} \\subset (V\\times V)\\setminus E,\n",
        "\\quad\n",
        "E^-_{\\text{test}} \\subset (V\\times V)\\setminus E,\n",
        "$$\n",
        "often created via random negative sampling.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Loss Function: Binary Cross-Entropy with Negative Sampling\n",
        "\n",
        "At each epoch, sample negatives $E^-_{\\text{train}}$ (commonly $|E^-_{\\text{train}}| = |E^+_{\\text{train}}|$). Define labels:\n",
        "$$\n",
        "y_{uv}=\n",
        "\\begin{cases}\n",
        "1 & (u,v)\\in E^+_{\\text{train}}\\\\\n",
        "0 & (u,v)\\in E^-_{\\text{train}}\n",
        "\\end{cases}\n",
        "$$\n",
        "and logits $s_{uv}$ from the decoder.\n",
        "\n",
        "The binary cross-entropy loss is:\n",
        "$$\n",
        "\\mathcal{L}_{\\text{BCE}}=\n",
        "-\\sum_{(u,v)\\in E^+_{\\text{train}}\\cup E^-_{\\text{train}}}\n",
        "\\left[\n",
        "y_{uv}\\log \\sigma(s_{uv}) + (1-y_{uv})\\log (1-\\sigma(s_{uv}))\n",
        "\\right].\n",
        "$$\n",
        "\n",
        "This objective trains the encoder (message passing) and decoder end-to-end.\n",
        "\n",
        "---\n",
        "\n",
        "### Evaluation Metrics\n",
        "\n",
        "We evaluate on $\\big(E^+_{\\text{val}}\\cup E^-_{\\text{val}}\\big)$ and $\\big(E^+_{\\text{test}}\\cup E^-_{\\text{test}}\\big)$ using ranking/classification metrics such as:\n",
        "- **ROC-AUC**\n",
        "- **Average Precision (AP)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-HWu2yB_Rw2Z",
      "metadata": {
        "id": "-HWu2yB_Rw2Z"
      },
      "source": [
        "---\n",
        "### The dataset\n",
        "#### Cora\n",
        "The dataset we will be using is Cora (https://graphsandnetworks.com/the-cora-dataset/). The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
        "In graph terms, each paper is a node and the citation-relationship determines directed edges between papers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "iz-hIPwBPyyN",
      "metadata": {
        "id": "iz-hIPwBPyyN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b51d069f-7d9e-4edb-e2d0-35ead2fc5714"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
            "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data(x=[2708, 1433], edge_index=[2, 8976], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[4488], edge_label_index=[2, 4488])\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import Planetoid\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Device:\", device)\n",
        "\n",
        "transform = T.Compose([\n",
        "    T.NormalizeFeatures(),\n",
        "    T.RandomLinkSplit(\n",
        "        num_val=0.05,\n",
        "        num_test=0.10,\n",
        "        is_undirected=True,\n",
        "        add_negative_train_samples=False,  # we will negative-sample each epoch\n",
        "    ),\n",
        "])\n",
        "\n",
        "dataset = Planetoid(root=\"data/Planetoid\", name=\"Cora\", transform=transform)\n",
        "train_data, val_data, test_data = dataset[0]\n",
        "\n",
        "train_data = train_data.to(device)\n",
        "val_data   = val_data.to(device)\n",
        "test_data  = test_data.to(device)\n",
        "\n",
        "print(train_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hQ_Jk5Toh3YV",
      "metadata": {
        "id": "hQ_Jk5Toh3YV"
      },
      "source": [
        "# Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nr7K6GHRhjmm",
      "metadata": {
        "id": "nr7K6GHRhjmm"
      },
      "source": [
        "What are the $\\psi$ and $\\phi$ functions for GIN? How do the message, aggregate, and update functions look like?\n",
        "\n",
        "[Answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LvGrMZJVZxBh",
      "metadata": {
        "id": "LvGrMZJVZxBh"
      },
      "source": [
        "#### Message Passing GNN (GIN) + MLP decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "Isatg7ViZq3n",
      "metadata": {
        "id": "Isatg7ViZq3n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.nn import GINConv\n",
        "from torch_geometric.utils import negative_sampling\n",
        "\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "class GINEncoder(nn.Module):\n",
        "    def __init__(self, in_channels: int, hidden: int = 128, out_channels: int = 128, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        \"\"\" Code the GIN! \"\"\"\n",
        "        self.conv1 = GINConv(\n",
        "            nn.Sequential(\n",
        "                nn.Linear(in_channels, hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden, hidden)\n",
        "            )\n",
        "        )\n",
        "        self.conv2 = GINConv(\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden, hidden),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden, out_channels)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.dropout = dropout\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EdgeDecoder(nn.Module):\n",
        "    \"\"\"MLP over pairwise node embeddings -> edge logit.\"\"\"\n",
        "    def __init__(self, z_dim: int, hidden: int = 128, dropout: float = 0.2):\n",
        "        super().__init__()   ,
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(2 * z_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, z, edge_label_index):\n",
        "        z_src = z[edge_label_index[0]]\n",
        "        z_dst = z[edge_label_index[1]]\n",
        "        z_pair = torch.cat([z_src, z_dst], dim=1)\n",
        "        return self.mlp(z_pair).view(-1)\n",
        "\n",
        "\n",
        "class LinkPredictor(nn.Module):\n",
        "    def __init__(self, in_channels, hidden=128, z_dim=128, dropout=0.2):\n",
        "        super().__init__()   ,
        "\n",
        "        self.encoder = GINEncoder(in_channels, hidden, z_dim, dropout)\n",
        "        self.decoder = EdgeDecoder(z_dim, hidden, dropout)\n",
        "\n",
        "    def encode(self, x, edge_index):\n",
        "        return self.encoder(x, edge_index)\n",
        "\n",
        "    def decode(self, z, edge_label_index):\n",
        "        return self.decoder(z, edge_label_index)\n",
        "\n",
        "model = LinkPredictor(in_channels=train_data.num_features, hidden=128, z_dim=128, dropout=0.2).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = nn.BCEWithLogitsLoss()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KukMBrq8Z5eb",
      "metadata": {
        "id": "KukMBrq8Z5eb"
      },
      "source": [
        "#### Train / Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "2Ourt5n9Z0ro",
      "metadata": {
        "id": "2Ourt5n9Z0ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1131fc09-0721-406d-ad97-1855983966ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 010 | Loss 0.6566 | Val AUC 0.6537 AP 0.7073 | Test AUC 0.6860 AP 0.7251\n",
            "Epoch 020 | Loss 0.5494 | Val AUC 0.6757 AP 0.7208 | Test AUC 0.6877 AP 0.7295\n",
            "Epoch 030 | Loss 0.5318 | Val AUC 0.6799 AP 0.7217 | Test AUC 0.6890 AP 0.7296\n",
            "Epoch 040 | Loss 0.5330 | Val AUC 0.6739 AP 0.7164 | Test AUC 0.6868 AP 0.7269\n",
            "Epoch 050 | Loss 0.5126 | Val AUC 0.6742 AP 0.7166 | Test AUC 0.6869 AP 0.7260\n",
            "Epoch 060 | Loss 0.5197 | Val AUC 0.6723 AP 0.7165 | Test AUC 0.6870 AP 0.7288\n",
            "Epoch 070 | Loss 0.5207 | Val AUC 0.6689 AP 0.7130 | Test AUC 0.6856 AP 0.7239\n",
            "Epoch 080 | Loss 0.4933 | Val AUC 0.6770 AP 0.7191 | Test AUC 0.6868 AP 0.7209\n",
            "Epoch 090 | Loss 0.5339 | Val AUC 0.6883 AP 0.7208 | Test AUC 0.6856 AP 0.7148\n",
            "Epoch 100 | Loss 0.4562 | Val AUC 0.7111 AP 0.7439 | Test AUC 0.7236 AP 0.7584\n",
            "Epoch 110 | Loss 0.3731 | Val AUC 0.7424 AP 0.7559 | Test AUC 0.7793 AP 0.7908\n",
            "Epoch 120 | Loss 0.3380 | Val AUC 0.7728 AP 0.7800 | Test AUC 0.8141 AP 0.8175\n",
            "Epoch 130 | Loss 0.3106 | Val AUC 0.8055 AP 0.8080 | Test AUC 0.8218 AP 0.8240\n",
            "Epoch 140 | Loss 0.3047 | Val AUC 0.8243 AP 0.8296 | Test AUC 0.8331 AP 0.8289\n",
            "Epoch 150 | Loss 0.3026 | Val AUC 0.8139 AP 0.8216 | Test AUC 0.8178 AP 0.8149\n",
            "Epoch 160 | Loss 0.2830 | Val AUC 0.8310 AP 0.8327 | Test AUC 0.8347 AP 0.8381\n",
            "Epoch 170 | Loss 0.2520 | Val AUC 0.8255 AP 0.8311 | Test AUC 0.8369 AP 0.8362\n",
            "Epoch 180 | Loss 0.2475 | Val AUC 0.8313 AP 0.8357 | Test AUC 0.8428 AP 0.8413\n",
            "Epoch 190 | Loss 0.2264 | Val AUC 0.8391 AP 0.8428 | Test AUC 0.8477 AP 0.8515\n",
            "Epoch 200 | Loss 0.2329 | Val AUC 0.8400 AP 0.8468 | Test AUC 0.8488 AP 0.8515\n",
            "Epoch 210 | Loss 0.2251 | Val AUC 0.8378 AP 0.8354 | Test AUC 0.8486 AP 0.8503\n",
            "Epoch 220 | Loss 0.2139 | Val AUC 0.8376 AP 0.8411 | Test AUC 0.8557 AP 0.8604\n",
            "Epoch 230 | Loss 0.2180 | Val AUC 0.8357 AP 0.8349 | Test AUC 0.8553 AP 0.8567\n",
            "Epoch 240 | Loss 0.2119 | Val AUC 0.8343 AP 0.8384 | Test AUC 0.8537 AP 0.8489\n",
            "Epoch 250 | Loss 0.2129 | Val AUC 0.8394 AP 0.8417 | Test AUC 0.8512 AP 0.8497\n",
            "Epoch 260 | Loss 0.2093 | Val AUC 0.8377 AP 0.8400 | Test AUC 0.8602 AP 0.8653\n",
            "Epoch 270 | Loss 0.2072 | Val AUC 0.8363 AP 0.8412 | Test AUC 0.8583 AP 0.8572\n",
            "Epoch 280 | Loss 0.2029 | Val AUC 0.8441 AP 0.8366 | Test AUC 0.8618 AP 0.8676\n",
            "Epoch 290 | Loss 0.2066 | Val AUC 0.8557 AP 0.8558 | Test AUC 0.8614 AP 0.8588\n",
            "Epoch 300 | Loss 0.1923 | Val AUC 0.8440 AP 0.8419 | Test AUC 0.8600 AP 0.8581\n",
            "Epoch 310 | Loss 0.1941 | Val AUC 0.8606 AP 0.8620 | Test AUC 0.8609 AP 0.8589\n",
            "Epoch 320 | Loss 0.2030 | Val AUC 0.8659 AP 0.8690 | Test AUC 0.8677 AP 0.8629\n",
            "Epoch 330 | Loss 0.2693 | Val AUC 0.8621 AP 0.8583 | Test AUC 0.8602 AP 0.8449\n",
            "Epoch 340 | Loss 0.2180 | Val AUC 0.8552 AP 0.8512 | Test AUC 0.8682 AP 0.8614\n",
            "Epoch 350 | Loss 0.2042 | Val AUC 0.8597 AP 0.8643 | Test AUC 0.8643 AP 0.8655\n",
            "Epoch 360 | Loss 0.1851 | Val AUC 0.8636 AP 0.8658 | Test AUC 0.8694 AP 0.8721\n",
            "Epoch 370 | Loss 0.1874 | Val AUC 0.8643 AP 0.8676 | Test AUC 0.8672 AP 0.8603\n",
            "Epoch 380 | Loss 0.1816 | Val AUC 0.8562 AP 0.8549 | Test AUC 0.8628 AP 0.8542\n",
            "Epoch 390 | Loss 0.1821 | Val AUC 0.8609 AP 0.8713 | Test AUC 0.8649 AP 0.8609\n",
            "Epoch 400 | Loss 0.1848 | Val AUC 0.8567 AP 0.8664 | Test AUC 0.8638 AP 0.8639\n",
            "Epoch 410 | Loss 0.1826 | Val AUC 0.8542 AP 0.8575 | Test AUC 0.8608 AP 0.8579\n",
            "Epoch 420 | Loss 0.1778 | Val AUC 0.8559 AP 0.8661 | Test AUC 0.8639 AP 0.8659\n",
            "Epoch 430 | Loss 0.1650 | Val AUC 0.8640 AP 0.8786 | Test AUC 0.8616 AP 0.8640\n",
            "Epoch 440 | Loss 0.1762 | Val AUC 0.8615 AP 0.8703 | Test AUC 0.8588 AP 0.8556\n",
            "Epoch 450 | Loss 0.1836 | Val AUC 0.8562 AP 0.8691 | Test AUC 0.8656 AP 0.8612\n",
            "Epoch 460 | Loss 0.1838 | Val AUC 0.8577 AP 0.8721 | Test AUC 0.8656 AP 0.8638\n",
            "Epoch 470 | Loss 0.1788 | Val AUC 0.8559 AP 0.8603 | Test AUC 0.8645 AP 0.8638\n",
            "Epoch 480 | Loss 0.1869 | Val AUC 0.8677 AP 0.8783 | Test AUC 0.8536 AP 0.8430\n",
            "Epoch 490 | Loss 0.1922 | Val AUC 0.8613 AP 0.8710 | Test AUC 0.8588 AP 0.8610\n",
            "Epoch 500 | Loss 0.1848 | Val AUC 0.8590 AP 0.8664 | Test AUC 0.8648 AP 0.8642\n",
            "\n",
            "Best Val AUC: 0.8677 | Val AP: 0.8783\n",
            "Final Test AUC: 0.8536 | Test AP: 0.8430\n"
          ]
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate(split_data):\n",
        "    model.eval()\n",
        "    z = model.encode(split_data.x, split_data.edge_index)\n",
        "    logits = model.decode(z, split_data.edge_label_index)\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()\n",
        "    y = split_data.edge_label.cpu().numpy()\n",
        "\n",
        "    auc = roc_auc_score(y, probs)\n",
        "    ap = average_precision_score(y, probs)\n",
        "    return auc, ap\n",
        "\n",
        "\n",
        "def train_one_epoch():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    z = model.encode(train_data.x, train_data.edge_index)\n",
        "\n",
        "    # Positive supervision edges provided by RandomLinkSplit\n",
        "    pos_edge_index = train_data.edge_label_index\n",
        "    num_pos = pos_edge_index.size(1)\n",
        "\n",
        "    # Fresh negative edges each epoch\n",
        "    neg_edge_index = negative_sampling(\n",
        "        edge_index=train_data.edge_index,\n",
        "        num_nodes=train_data.num_nodes,\n",
        "        num_neg_samples=num_pos,\n",
        "        method='sparse',\n",
        "    )\n",
        "\n",
        "    edge_label_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
        "    edge_label = torch.cat([\n",
        "        torch.ones(num_pos, device=device),\n",
        "        torch.zeros(neg_edge_index.size(1), device=device),\n",
        "    ], dim=0)\n",
        "\n",
        "    logits = model.decode(z, edge_label_index)\n",
        "    loss = criterion(logits, edge_label)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return float(loss)\n",
        "\n",
        "\n",
        "best_val_auc = -1.0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(1, 501):\n",
        "    loss = train_one_epoch()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        val_auc, val_ap = evaluate(val_data)\n",
        "        test_auc, test_ap = evaluate(test_data)\n",
        "        print(f\"Epoch {epoch:03d} | Loss {loss:.4f} | Val AUC {val_auc:.4f} AP {val_ap:.4f} | Test AUC {test_auc:.4f} AP {test_ap:.4f}\")\n",
        "\n",
        "        if val_auc > best_val_auc:\n",
        "            best_val_auc = val_auc\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "\n",
        "val_auc, val_ap = evaluate(val_data)\n",
        "test_auc, test_ap = evaluate(test_data)\n",
        "print(f\"\\nBest Val AUC: {val_auc:.4f} | Val AP: {val_ap:.4f}\")\n",
        "print(f\"Final Test AUC: {test_auc:.4f} | Test AP: {test_ap:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qz_lmO-kDAC9",
      "metadata": {
        "id": "qz_lmO-kDAC9"
      },
      "source": [
        "**Note**\n",
        "Golden scores: These scores help you assess if you've coded up the GIN correctly. Try to match them -- or beat them!\n",
        "\n",
        "`Best Val AUC: 0.8501 | Val AP: 0.8558\n",
        "Final Test AUC: 0.8533 | Test AP: 0.8483`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "znsiID6hbu2m",
      "metadata": {
        "id": "znsiID6hbu2m"
      },
      "source": [
        "#ğŸ¥ˆ Part 2: Expressivity of GNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4pyYuMPQfX8i",
      "metadata": {
        "id": "4pyYuMPQfX8i"
      },
      "source": [
        "## Weisfeilerâ€“Lehman (1-WL Hashing)\n",
        "\n",
        "We will explore the expressivity of GNNs with implementing the **1-dimensional Weisfeilerâ€“Lehman (1-WL)** algorithm (also called **color refinement**) and use it to study the **applicability and limitations** of message passing GNNs for distinguishing graphs.\n",
        "\n",
        "\n",
        "\n",
        "### Why does WL matters for GNN expressivity?\n",
        "\n",
        "Most common GNN architectures (GCN, GraphSAGE, GAT, GIN, and many MPNNs) follow the **message passing** paradigm:\n",
        "\n",
        "- each node updates its representation using a *permutation-invariant* aggregation of its neighborsâ€™ representations,\n",
        "- repeated over $L$ layers.\n",
        "\n",
        "The 1-WL algorithm is a classical procedure that iteratively refines **discrete node labels** based on neighbor labels. The update structure is closely aligned with message passing. This connection motivates a key principle:\n",
        "\n",
        "- **1-WL is an upper bound on the distinguishing power of standard message-passing GNNs.**\n",
        "This means that if **1-WL cannot distinguish** two graphs, then a standard message passing GNN (without extra node identifiers/positional encodings) cannot be guaranteed to distinguish them either. If **1-WL does distinguish** two graphs, then a sufficiently expressive message passing GNN (with injective aggregation/update) *can* separate them in principle.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## The 1-WL color-refinement algorithm\n",
        "\n",
        "Let $G=(V,E)$ be a graph. Each node $i \\in V$ starts with a discrete label (color) $c_i^{(0)}$.\n",
        "If graphs have node labels, use those labels as $c_i^{(0)}$. Otherwise, a common default is **all nodes share the same initial label** (or optionally use node degree as initialization).\n",
        "\n",
        "### Iterative refinement\n",
        "\n",
        "For iterations $t=0,1,\\dots,T-1$, update each node label using its current label and the **multiset** of neighbor labels:\n",
        "$$\n",
        "c_i^{(t+1)} \\;=\\; \\mathrm{HASH}\\Big( c_i^{(t)}, \\text{multiset}\\big(\\{ c_j^{(t)} : j \\in \\mathcal{N}(i) \\}\\big),\n",
        "$$\n",
        "where:\n",
        "- $\\mathcal{N}(i)$ is the set of neighbors of node $i$,\n",
        "- $\\text{multiset}(\\cdot)$ denotes a multiset (neighbor labels can repeat),\n",
        "- `HASH` is a deterministic mapping from *signatures* to new discrete labels\n",
        "\n",
        "To represent the neighbor multiset deterministically, sort it and assign a new integer label to each unique signature. Then, replace node labels by these new integers to obtain $c_i^{(t+1)}$.\n",
        "\n",
        "The algorithm terminates when the labels stabilize (no changes), or\n",
        "a fixed number of rounds \\(T\\) is reached.\n",
        "\n",
        "In practice, using a small \\(T\\) is useful when you want to compare against \\(L\\)-layer GNNs.\n",
        "\n",
        "\n",
        "If for some \\(t\\),\n",
        "$$\n",
        "C^{(t)}(G)\\neq C^{(t)}(H),\n",
        "$$\n",
        "then 1-WL distinguishes \\(G\\) and \\(H\\), meaning they are definitely **non-isomorphic**.\n",
        "\n",
        "**Note:** identical WL fingerprints (sorted $C_j$) do **not** guarantee isomorphism, as 1-WL is not complete.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-1497Y1vsezZ",
      "metadata": {
        "id": "-1497Y1vsezZ"
      },
      "source": [
        "## 1-WL as a test for GNN Expressivity\n",
        "\n",
        "The **1-dimensional Weisfeilerâ€“Lehman (1-WL) algorithm**, often called **color refinement**, serves as the theoretical \"gold standard\" for measuring the expressivity of Graph Neural Networks. By implementing 1-WL, we can rigorously study the fundamental limitations of message passing GNNs in distinguishing non-isomorphic graph structures.\n",
        "\n",
        "### The Link Between WL and GNNs\n",
        "\n",
        "Most modern GNN architectures (e.g., GCN, GAT, GraphSAGE) are categorized as **Message Passing Neural Networks (MPNNs)**. Their update mechanism mirrors the WL process:\n",
        "* **Local Aggregation:** Each node updates its state using a *permutation-invariant* function over its neighborhood.\n",
        "* **Iterative Refinement:** Representations are refined over $L$ successive layers.\n",
        "\n",
        "\n",
        "\n",
        "**The Key Principle:**\n",
        "The 1-WL test provides a **theoretical upper bound** on the distinguishing power of standard MPNNs.\n",
        "* If 1-WL cannot distinguish two graphs $G$ and $H$, no standard MPNN can distinguish them, regardless of its depth or hidden dimension.\n",
        "* A GNN achieves \"WL-equivalent\" power only if its aggregation and update functions are **injective** (e.g., the Graph Isomorphism Network, or GIN).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UfMTewvx4WpX",
      "metadata": {
        "id": "UfMTewvx4WpX"
      },
      "source": [
        "The graphs below are non-isomorphic but have the same colouring.\n",
        "<img src=\"https://github.com/metalisia/gdl_images/blob/main/1wl.png?raw=1\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oowDZS9S4dFG",
      "metadata": {
        "id": "oowDZS9S4dFG"
      },
      "source": [
        "\n",
        "## The 1-WL Algorithm: Step-by-Step\n",
        "\n",
        "Consider a graph $G=(V, E)$. The algorithm proceeds by iteratively refining discrete node labels (colors).\n",
        "\n",
        "### 1. Initialization\n",
        "Each node $i \\in V$ is assigned an initial color $c_i^{(0)}$.\n",
        "* If the dataset provides node features/labels, use those.\n",
        "* If the graph is unlabeled, initialize all nodes with the same constant value, $c_i^{(0)} = 1$.\n",
        "\n",
        "### 2. Iterative Color Refinement\n",
        "For each iteration $t = 0, 1, \\dots, T-1$, we update the color of node $i$ based on its current state and the **multiset** of its neighbors' colors:\n",
        "\n",
        "$$c_i^{(t+1)} = \\text{HASH} \\left( c_i^{(t)}, \\left\\{\\!\\left\\{ c_j^{(t)} \\mid j \\in \\mathcal{N}(i) \\right\\}\\!\\right\\} \\right)$$\n",
        "\n",
        "Where:\n",
        "* $\\{\\!\\{ \\cdot \\}\\!\\}$ denotes a **multiset**, capturing both the values and the frequencies of neighbor labels.\n",
        "* $\\text{HASH}$ is an injective function that maps each unique (node color, neighbor multiset) pair to a unique new integer.\n",
        "\n",
        "### 3. Practical Implementation\n",
        "In code, the \"Hashing\" step is typically performed by:\n",
        "1.  Collecting the labels of all neighbors.\n",
        "2.  Sorting the labels to ensure the representation is **permutation-invariant**.\n",
        "3.  Concatenating the nodeâ€™s own label with the sorted neighbor list to create a unique \"signature.\"\n",
        "4.  Mapping these signatures to new, smaller integers using a hash table.\n",
        "\n",
        "---\n",
        "\n",
        "## Graph Distinguishability and Isomorphism\n",
        "\n",
        "After $T$ iterations, we define the **graph fingerprint** as the sorted multiset of all node colors: $\\text{f}(G) = \\{\\!\\{ c_i^{(T)} \\mid i \\in V \\}\\!\\}$.\n",
        "\n",
        "* **Non-Isomorphism Check:** If $\\text{f}(G) \\neq \\text{f}(H)$, the graphs are guaranteed to be **non-isomorphic**.\n",
        "* **The 1-WL Limit:** If $\\text{f}(G) = \\text{f}(H)$, the graphs *might* be isomorphic. However, 1-WL is \"incomplete\"; it famously fails to distinguish certain structures like circular skip links or regular graphs with the same degree."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "iVN_FCZrzZ6n",
      "metadata": {
        "id": "iVN_FCZrzZ6n"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "\n",
        "\n",
        "def wl1_colors_nx(G, num_iters=5, init=\"uniform\"):\n",
        "    nodes = list(G.nodes())\n",
        "    if init == \"uniform\":\n",
        "        colors = {i: 0 for i in nodes}\n",
        "    elif init == \"degree\":\n",
        "        colors = {i: G.degree[i] for i in nodes}\n",
        "    else:\n",
        "        raise ValueError(\"init must be 'uniform' or 'degree'\")\n",
        "\n",
        "    colors_by_iter = [colors.copy()]\n",
        "    for _ in range(num_iters):\n",
        "        sigs = {}\n",
        "        for i in nodes:\n",
        "            neigh_cols = sorted(colors[j] for j in G.neighbors(i))\n",
        "            sigs[i] = (colors[i], tuple(neigh_cols))\n",
        "\n",
        "        uniq = sorted(set(sigs.values()))\n",
        "        mapping = {sig: idx for idx, sig in enumerate(uniq)}\n",
        "        colors = {i: mapping[sigs[i]] for i in nodes}\n",
        "        colors_by_iter.append(colors.copy())\n",
        "    return colors_by_iter\n",
        "\n",
        "def wl_fingerprint(colors_by_iter):\n",
        "    fp = []\n",
        "    for colors in colors_by_iter:\n",
        "        hist = Counter(colors.values())\n",
        "        fp.append(tuple(sorted(hist.items())))\n",
        "    return tuple(fp)\n",
        "\n",
        "def wl_distinguishes(G, H, T=5, init=\"uniform\"):\n",
        "    fpG = wl_fingerprint(wl1_colors_nx(G, num_iters=T, init=init))\n",
        "    fpH = wl_fingerprint(wl1_colors_nx(H, num_iters=T, init=init))\n",
        "    return fpG != fpH\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aIJL7VUCsLCo",
      "metadata": {
        "id": "aIJL7VUCsLCo"
      },
      "source": [
        "## From 1-WL to Message Passing: The Expressivity Connection\n",
        "\n",
        "A typical Message Passing Neural Network (MPNN) update follows the general form:\n",
        "$$\n",
        "h_i^{(t+1)} = \\phi\\Big(h_i^{(t)},\\ \\mathrm{AGG}\\{ \\psi(h_i^{(t)}, h_j^{(t)}) : j\\in \\mathcal{N}(i)\\}\\Big)\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- **$\\text{AGG}$**: A permutation-invariant operator (e.g., $\\text{SUM}, \\text{MEAN}, \\text{MAX}$).\n",
        "- **$\\psi$ and $\\phi$**: Learnable functions (typically MLPs).\n",
        "\n",
        "### The Parallel to 1-WL\n",
        "The structural similarity between MPNNs and 1-WL is profound:\n",
        "* **Discrete vs. Continuous:** 1-WL updates **discrete labels** via hashing, while GNNs update **continuous embeddings** via neural layers.\n",
        "* **Injective Mapping:** If a GNNâ€™s aggregation and update functions are **injective**, the GNN can theoretically reachâ€”but never exceedâ€”the graph-distinguishing power of the 1-WL test.\n",
        "\n",
        "\n",
        "\n",
        "### Expressivity of Common GNN Architectures\n",
        "\n",
        "* **GIN (Graph Isomorphism Network):** Specifically designed to be **1-WL equivalent**. By using **sum aggregation** and a MLP (based on the Universal Approximation Theorem), GIN can represent injective functions over multisets.\n",
        "* **GCN / Mean Aggregators:** Inherently **less expressive** than 1-WL. Mean aggregation can produce identical outputs for different multisets (e.g., a neighborhood of $\\{1, 1\\}$ vs. $\\{1, 1, 1, 1\\}$), failing to capture structural cardinality.\n",
        "* **GAT (Graph Attention Network):** While powerful for feature weighting, GAT is still bounded by 1-WL. It can still suffer from \"collisions\" where different local structures result in the same aggregated representation.\n",
        "\n",
        "\n",
        "\n",
        "## Capabilities and Fundamental Limitations of 1-WL\n",
        "\n",
        "### Strengths of the 1-WL Test\n",
        "- **Efficiency:** Provides a fast, deterministic refinement of node labels.\n",
        "- **Local Motif Detection:** Effectively distinguishes graphs with differing local connectivity patterns.\n",
        "- **Soundness:** If 1-WL distinguishes two graphs, they are **guaranteed** to be non-isomorphic.\n",
        "\n",
        "### The Failure Case: Regularity and Symmetry\n",
        "The primary limitation of 1-WL is its inability to distinguish graphs where nodes \"see\" identical local environments due to high symmetry or regularity.\n",
        "\n",
        "**Classic Counterexample:**\n",
        "* **Graph $G$**: A single 6-node cycle ($C_6$).\n",
        "* **Graph $H$**: Two disjoint 3-node triangles ($C_3 \\cup C_3$).\n",
        "\n",
        "\n",
        "\n",
        "In both cases, every node is **2-regular** (has exactly 2 neighbors). If all nodes start with the same label, 1-WL refinement will never break symmetry; every node in both $G$ and $H$ will receive the exact same sequence of labels. 1-WL will conclude they are \"possibly isomorphic,\" even though their global structures are completely different.\n",
        "\n",
        "### Consequence for GNNs\n",
        "Standard MPNNs without **positional encodings** or **unique node IDs** inherit these 1-WL failures. A standard GCN, for instance, would produce identical graph-level embeddings for $C_6$ and $C_3 \\cup C_3$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Bi1mRqm1zhTJ",
      "metadata": {
        "id": "Bi1mRqm1zhTJ"
      },
      "source": [
        "In the code below, we compare outputs of the 1-WL test and GIN architecture on the same graphs.\n",
        "- Case A) Graph Isomorphism: same graph $\\mathbb{G}$\n",
        "- Case B) Graph Non-Isomorphism: $\\mathbb{G}$ !=  $\\mathbb{H}$\n",
        "- Case C) Study case on failure mode: $\\mathbb{C_6}$ (hexagon) != $\\mathbb{C_3}$ U $\\mathbb{C_3}$ (two triangles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "vgkbFf7_zTWE",
      "metadata": {
        "id": "vgkbFf7_zTWE"
      },
      "outputs": [],
      "source": [
        "def nx_to_pyg(G, x_mode=\"ones\"):\n",
        "    n = G.number_of_nodes()\n",
        "    assert set(G.nodes()) == set(range(n)), \"Expected node labels 0..n-1\"\n",
        "\n",
        "    edges = list(G.edges())\n",
        "    edge_index = torch.tensor(edges + [(v, u) for (u, v) in edges], dtype=torch.long).t().contiguous()\n",
        "\n",
        "    if x_mode == \"ones\":\n",
        "        x = torch.ones((n, 1), dtype=torch.float)\n",
        "    elif x_mode == \"degree\":\n",
        "        x = torch.tensor([G.degree[i] for i in range(n)], dtype=torch.float).view(-1, 1)\n",
        "    else:\n",
        "        raise ValueError(\"x_mode must be 'ones' or 'degree'\")\n",
        "\n",
        "    return Data(x=x, edge_index=edge_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "-YGeqQpswgND",
      "metadata": {
        "id": "-YGeqQpswgND"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "from torch_geometric.data import Data\n",
        "import networkx as nx\n",
        "from collections import Counter\n",
        "import random\n",
        "\n",
        "# Graph-level wrapper: encoder + pooling ---\n",
        "class GINGraphModel(nn.Module):\n",
        "    def __init__(self, in_channels=1, hidden=128, out_channels=128, dropout=0.0, pool=\"add\"):\n",
        "        super().__init__()\n",
        "        self.encoder = GINEncoder(in_channels, hidden, out_channels, dropout)\n",
        "        self.pool = pool\n",
        "\n",
        "    def forward(self, data: Data):\n",
        "        z = self.encoder(data.x, data.edge_index)  # [N, out_channels]\n",
        "        batch = getattr(data, \"batch\", None)\n",
        "        if batch is None:\n",
        "            batch = torch.zeros(z.size(0), dtype=torch.long, device=z.device)\n",
        "\n",
        "        if self.pool == \"add\":\n",
        "            g = global_add_pool(z, batch)\n",
        "        else:\n",
        "            raise ValueError(\"Only pool='add' implemented here.\")\n",
        "\n",
        "        return g.squeeze(0)  # [out_channels] for a single graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "B3XlSyf7x3Hi",
      "metadata": {
        "id": "B3XlSyf7x3Hi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cad6a0f-c175-4ce0-ada0-a58e7eeaf70b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Case A: Isomorphic (C6 vs relabeled C6).\n",
            "  ** Expected outcome: 1-WL distinguishes is False (same graph), GIN embedding distance ~ 0.00\n",
            "1-WL distinguishes? False (T=5, init=uniform)\n",
            "GIN graph embedding L2 distance: 0.000000 (x_mode=ones)\n",
            "\n",
            "Case B1: Non-isomorphic (C6 vs P6).\n",
            "  ** Expected outcome: 1-WL distinguishes is True (graphs are different), GIN embedding distance > 0.00\n",
            "1-WL distinguishes? True (T=5, init=uniform)\n",
            "GIN graph embedding L2 distance: 5.320123 (x_mode=ones)\n",
            "\n",
            "Case B2: Non-isomorphic (C6 vs 2Ã—C3)\n",
            "  ** Expected outcome: 1-WL distinguishes is False (failure mode), GIN embedding distance ~ 0.00\n",
            "1-WL distinguishes? False (T=5, init=uniform)\n",
            "GIN graph embedding L2 distance: 0.000000 (x_mode=ones)\n",
            "\n",
            "Case B2 with degree init/features\n",
            "1-WL distinguishes? False (T=5, init=degree)\n",
            "GIN graph embedding L2 distance: 0.000000 (x_mode=degree)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "torch.manual_seed(42)\n",
        "\n",
        "model = GINGraphModel(in_channels=1, hidden=128, out_channels=128, dropout=0.0, pool=\"add\").to(device)\n",
        "model.eval()\n",
        "\n",
        "def gin_embed(G, x_mode=\"ones\"):\n",
        "    data = nx_to_pyg(G, x_mode=x_mode).to(device)\n",
        "    with torch.no_grad():\n",
        "        g = model(data)\n",
        "    return g\n",
        "\n",
        "def compare_pair(name, A, B, wl_T=5, wl_init=\"uniform\", x_mode=\"ones\"):\n",
        "    wl_diff = wl_distinguishes(A, B, T=wl_T, init=wl_init)\n",
        "    gA = gin_embed(A, x_mode=x_mode)\n",
        "    gB = gin_embed(B, x_mode=x_mode)\n",
        "    dist = torch.norm(gA - gB, p=2).item()\n",
        "\n",
        "    print(f\"\\n{name}\")\n",
        "    print(f\"1-WL distinguishes? {wl_diff} (T={wl_T}, init={wl_init})\")\n",
        "    print(f\"GIN graph embedding L2 distance: {dist:.6f} (x_mode={x_mode})\")\n",
        "\n",
        "# Case A: isomorphic pair (C6 vs relabeled C6)\n",
        "G0 = nx.cycle_graph(6)\n",
        "perm = list(range(6))\n",
        "random.seed(0)\n",
        "random.shuffle(perm)\n",
        "mapping = {i: perm[i] for i in range(6)}\n",
        "G0_iso = nx.relabel_nodes(G0, mapping)\n",
        "\n",
        "# Case B1: non-iso where WL succeeds (C6 vs P6)\n",
        "G_cycle = nx.cycle_graph(6)\n",
        "G_path = nx.path_graph(6)\n",
        "\n",
        "# Case B2: non-iso where WL fails (C6 vs two triangles)\n",
        "G_two_tri = nx.disjoint_union(nx.cycle_graph(3), nx.cycle_graph(3))\n",
        "\n",
        "compare_pair(\"Case A: Isomorphic (C6 vs relabeled C6).\\n  ** Expected outcome: 1-WL distinguishes is False (same graph), GIN embedding distance ~ 0.00\", G0, G0_iso, wl_T=5, wl_init=\"uniform\", x_mode=\"ones\")\n",
        "compare_pair(\"Case B1: Non-isomorphic (C6 vs P6).\\n  ** Expected outcome: 1-WL distinguishes is True (graphs are different), GIN embedding distance > 0.00\", G_cycle, G_path, wl_T=5, wl_init=\"uniform\", x_mode=\"ones\")\n",
        "compare_pair(\"Case B2: Non-isomorphic (C6 vs 2Ã—C3)\\n  ** Expected outcome: 1-WL distinguishes is False (failure mode), GIN embedding distance ~ 0.00\", G_cycle, G_two_tri, wl_T=5, wl_init=\"uniform\", x_mode=\"ones\")\n",
        "\n",
        "# Optional: show what happens if you give both WL and GIN degree initialization\n",
        "compare_pair(\"Case B2 with degree init/features\", G_cycle, G_two_tri, wl_T=5, wl_init=\"degree\", x_mode=\"degree\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lCYicTMN3189",
      "metadata": {
        "id": "lCYicTMN3189"
      },
      "source": [
        "#ğŸ¥‰ Part 3: Equivariance and Invariance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "g2rRRunU6i4v",
      "metadata": {
        "id": "g2rRRunU6i4v"
      },
      "source": [
        "## Testing for Permutation Invariance and Equivariance\n",
        "\n",
        "\n",
        "- A **GNN <ins>layer**</ins> is **equivariant** to permutations of the set of nodes in the graph; i.e. as we permute the nodes, the node features produced by the GNN must permute accordingly.\n",
        "- A **GNN <ins>model**</ins> for graph-level property prediction is **invariant** to the permutations of the set of nodes in the graph; i.e. as we permute the nodes, the graph-level property remains unchanged.\n",
        "\n",
        "A permutation is an **ordering of the nodes** in a graph. In general, there is **no canonical way** of assigning an ordering of the nodes, unlike textual or image data, and we need to ensure that our models are able to principaly handle this **lack of canonical ordering** or permutation of graph nodes.\n",
        "\n",
        "### Formalism\n",
        "\n",
        "Let us try to formalise these notions of permutation invariance and equivariance via matrix notation (it is easier that way).\n",
        "\n",
        "- Let $\\mathbf{H} \\in \\mathbb{R}^{n \\times d}$ be a matrix of node features for a given molecular graph, where $n$ is the number of nodes/atoms and each row $h_i$ is the $d$-dimensional feature for node $i$.\n",
        "- Let $\\mathbf{A} \\in \\mathbb{R}^{n \\times n}$ be the adjacency matrix where each entry denotes $a_{ij}$ the presence or absence of an edge between nodes $i$ and $j$.\n",
        "- Let $\\mathbf{F}(\\mathbf{H}, \\mathbf{A}): \\mathbb{R}^{n \\times d} \\times \\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{R}^{n \\times d}$ be a **GNN <ins>layer**</ins> that takes as input the node features and adjacency matrix, and returns the **updated node features**.\n",
        "- Let $f(\\mathbf{H}, \\mathbf{A}): \\mathbb{R}^{n \\times d} \\times \\mathbb{R}^{n \\times n} \\rightarrow \\mathbb{R}$ be a **GNN <ins>model**</ins> that takes as input the node features and adjacency matrix, and returns the **predicted graph-level property**.\n",
        "- Let $\\mathbf{P} \\in \\mathbb{R}^{n \\times n}$ be a **[permutation matrix](https://en.wikipedia.org/wiki/Permutation_matrix)** which has exactly one 1 in every row and column, and 0s elsewhere. Left-multipying $\\mathbf{P}$ with a matrix changes the ordering of the rows of the matrix.\n",
        "\n",
        "### Permuation Equivariance\n",
        "\n",
        "The GNN <ins>layer</ins> $\\mathbf{F}$ is **permuation equivariant** as follows:\n",
        "$$\n",
        "\\mathbf{F}(\\mathbf{PH}, \\mathbf{PAP^T}) = \\mathbf{P} \\ \\mathbf{F}(\\mathbf{H}, \\mathbf{A}).\n",
        "$$\n",
        "\n",
        "Another way to formulate the above could be: (1) Consider the updated node features $\\mathbf{H'} = \\mathbf{F}(\\mathbf{H}, \\mathbf{A})$. (2) Applying any permutation matrix $\\mathbf{P}$ to the input of the GNN layer $\\mathbf{F}$ should produce the same result as applying the same permutation on $\\mathbf{H'}$:\n",
        "$$\n",
        "\\mathbf{F}(\\mathbf{PH}, \\mathbf{PAP^T}) = \\mathbf{P} \\ \\mathbf{H'}\n",
        "$$\n",
        "\n",
        "### Permuation Invariance\n",
        "\n",
        "The GNN <ins>model</ins> $f$ for graph-level prediction is **permutation invariant** as follows:\n",
        "$$\n",
        "f(\\mathbf{PH}, \\mathbf{PAP^T}) = f(\\mathbf{H}, \\mathbf{A}).\n",
        "$$\n",
        "\n",
        "Another way to formulate the above could be: (1) Consider the predicted molecular property $\\mathbf{\\hat y} = f(\\mathbf{H}, \\mathbf{A})$. (2) Applying any permutation matrix $\\mathbf{P}$ to the input of the GNN model $f$ should produce the same result as not applying it:\n",
        "$$\n",
        "f(\\mathbf{PH}, \\mathbf{PAP^T}) = \\mathbf{\\hat y}.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c4k9vAM56kLF",
      "metadata": {
        "id": "c4k9vAM56kLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18596b68-77a9-444a-dadf-c6984cc410ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Single-shuffle sanity check (consistent permutation):\n",
            "Node equivariance error (max abs): 0.0\n",
            "Edge equivariance error (max abs): 0.0\n",
            "Graph invariance error (max abs): 5.960464477539063e-08\n",
            "\n",
            "Single-shuffle sanity check (inconsistent permutation):\n",
            "Graph invariance error (max abs) [should be > 0]: 0.09747634828090668\n",
            "\n",
            "Stress test: CONSISTENT shuffling (should be ~0 within tolerance)\n",
            "Node equivariance error  (min/med/max): (0.0, 0.0, 0.0)\n",
            "Edge equivariance error  (min/med/max): (0.0, 0.0, 0.0)\n",
            "Graph invariance error   (min/med/max): (2.9802322387695312e-08, 5.960464477539063e-08, 1.1920928955078125e-07)\n",
            "\n",
            "Stress test: INCONSISTENT shuffling (graph invariance should break)\n",
            "Graph invariance error   (min/med/max): (0.0624094232916832, 0.10102346539497375, 0.14415249228477478)\n"
          ]
        }
      ],
      "source": [
        "# Toy graph generator\n",
        "def make_toy_graph(num_nodes=12, feat_dim=16):\n",
        "    # Ring + a few chords, made undirected by adding both directions\n",
        "    edges = []\n",
        "    for i in range(num_nodes):\n",
        "        j = (i + 1) % num_nodes\n",
        "        edges.append((i, j))\n",
        "        edges.append((j, i))\n",
        "    for i in range(0, num_nodes, 3):\n",
        "        j = (i + 4) % num_nodes\n",
        "        edges.append((i, j))\n",
        "        edges.append((j, i))\n",
        "\n",
        "    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "    x = torch.randn(num_nodes, feat_dim)\n",
        "    return Data(x=x, edge_index=edge_index)\n",
        "\n",
        "\n",
        "# Permutation helpers\n",
        "def consistent_permute(data: Data, perm: torch.Tensor):\n",
        "    \"\"\"\n",
        "    perm: new_index -> old_index\n",
        "    Returns:\n",
        "      data_perm: permuted graph\n",
        "      invperm: old_index -> new_index\n",
        "    \"\"\"\n",
        "    N = data.num_nodes\n",
        "    invperm = torch.empty_like(perm)\n",
        "    invperm[perm] = torch.arange(N, device=perm.device)\n",
        "\n",
        "    x_perm = data.x[perm]\n",
        "    edge_index_perm = invperm[data.edge_index]  # relabel endpoints\n",
        "    return Data(x=x_perm, edge_index=edge_index_perm), invperm\n",
        "\n",
        "def inconsistent_permute(data: Data, perm: torch.Tensor):\n",
        "    \"\"\"Shuffles node features but does NOT relabel edges (breaks correctness).\"\"\"\n",
        "    x_perm = data.x[perm]\n",
        "    return Data(x=x_perm, edge_index=data.edge_index)\n",
        "\n",
        "\n",
        "# Metrics: equivariance/invariance errors\n",
        "@torch.no_grad()\n",
        "def compute_reference(model, data):\n",
        "    z = model.encode(data.x, data.edge_index)\n",
        "    batch = torch.zeros(data.num_nodes, dtype=torch.long, device=data.x.device)\n",
        "    g = global_mean_pool(z, batch).squeeze(0)\n",
        "\n",
        "    # fixed edge queries for edge-level equivariance\n",
        "    edge_label_index = torch.tensor([[0, 1, 2, 3, 4],\n",
        "                                     [5, 6, 7, 8, 9]], dtype=torch.long, device=data.x.device)\n",
        "    logits = model.decode(z, edge_label_index)\n",
        "    return z, g, edge_label_index, logits\n",
        "\n",
        "@torch.no_grad()\n",
        "def measure_errors(model, data_ref, z_ref, g_ref, edge_label_index_ref, logits_ref, data_perm, invperm=None):\n",
        "    \"\"\"\n",
        "    Returns (node_equiv_error, edge_equiv_error, graph_invar_error).\n",
        "    For consistent perms, provide invperm; for inconsistent, leave invperm=None.\n",
        "    \"\"\"\n",
        "    z_p = model.encode(data_perm.x, data_perm.edge_index)\n",
        "\n",
        "    # Graph invariance error\n",
        "    batch = torch.zeros(data_perm.num_nodes, dtype=torch.long, device=data_perm.x.device)\n",
        "    g_p = global_mean_pool(z_p, batch).squeeze(0)\n",
        "    graph_err = (g_p - g_ref).abs().max().item()\n",
        "\n",
        "    # Node equivariance + edge equivariance only make sense with consistent permutation\n",
        "    if invperm is not None:\n",
        "        # Recover perm from invperm (perm is inverse mapping)\n",
        "        perm = torch.empty_like(invperm)\n",
        "        perm[invperm] = torch.arange(invperm.numel(), device=invperm.device)\n",
        "\n",
        "        node_err = (z_p - z_ref[perm]).abs().max().item()\n",
        "\n",
        "        edge_label_index_p = invperm[edge_label_index_ref]\n",
        "        logits_p = model.decode(z_p, edge_label_index_p)\n",
        "        edge_err = (logits_p - logits_ref).abs().max().item()\n",
        "    else:\n",
        "        node_err = float(\"nan\")\n",
        "        edge_err = float(\"nan\")\n",
        "\n",
        "    return node_err, edge_err, graph_err\n",
        "\n",
        "def summarize(vals):\n",
        "    vals = [v for v in vals if not (isinstance(v, float) and (v != v))]  # drop NaNs\n",
        "    if not vals:\n",
        "        return None\n",
        "    t = torch.tensor(vals)\n",
        "    return float(t.min()), float(t.median()), float(t.max())\n",
        "\n",
        "\n",
        "# Demo\n",
        "feat_dim = 16\n",
        "model = LinkPredictor(in_channels=feat_dim, hidden=64, z_dim=64, dropout=0.2).to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "data = make_toy_graph(num_nodes=12, feat_dim=feat_dim).to(device)\n",
        "\n",
        "z_ref, g_ref, edge_label_index_ref, logits_ref = compute_reference(model, data)\n",
        "\n",
        "print(\"\\nSingle-shuffle sanity check (consistent permutation):\")\n",
        "perm = torch.randperm(data.num_nodes, device=device)\n",
        "data_p, invperm = consistent_permute(data, perm)\n",
        "node_err, edge_err, graph_err = measure_errors(\n",
        "    model, data, z_ref, g_ref, edge_label_index_ref, logits_ref, data_p, invperm=invperm\n",
        ")\n",
        "print(\"Node equivariance error (max abs):\", node_err)\n",
        "print(\"Edge equivariance error (max abs):\", edge_err)\n",
        "print(\"Graph invariance error (max abs):\", graph_err)\n",
        "\n",
        "print(\"\\nSingle-shuffle sanity check (inconsistent permutation):\")\n",
        "perm = torch.randperm(data.num_nodes, device=device)\n",
        "data_bad = inconsistent_permute(data, perm).to(device)\n",
        "node_err_bad, edge_err_bad, graph_err_bad = measure_errors(\n",
        "    model, data, z_ref, g_ref, edge_label_index_ref, logits_ref, data_bad, invperm=None\n",
        ")\n",
        "print(\"Graph invariance error (max abs) [should be > 0]:\", graph_err_bad)\n",
        "\n",
        "\n",
        "# Stress test over many shuffles\n",
        "def run_trials(num_trials=50, consistent=True):\n",
        "    node_errs, edge_errs, graph_errs = [], [], []\n",
        "    for _ in range(num_trials):\n",
        "        perm = torch.randperm(data.num_nodes, device=device)\n",
        "        if consistent:\n",
        "            data_p, invperm = consistent_permute(data, perm)\n",
        "            ne, ee, ge = measure_errors(\n",
        "                model, data, z_ref, g_ref, edge_label_index_ref, logits_ref, data_p, invperm=invperm\n",
        "            )\n",
        "        else:\n",
        "            data_p = inconsistent_permute(data, perm).to(device)\n",
        "            ne, ee, ge = measure_errors(\n",
        "                model, data, z_ref, g_ref, edge_label_index_ref, logits_ref, data_p, invperm=None\n",
        "            )\n",
        "\n",
        "        node_errs.append(ne)\n",
        "        edge_errs.append(ee)\n",
        "        graph_errs.append(ge)\n",
        "\n",
        "    return summarize(node_errs), summarize(edge_errs), summarize(graph_errs)\n",
        "\n",
        "print(\"\\nStress test: CONSISTENT shuffling (should be ~0 within tolerance)\")\n",
        "ne_s, ee_s, ge_s = run_trials(num_trials=100, consistent=True)\n",
        "print(\"Node equivariance error  (min/med/max):\", ne_s)\n",
        "print(\"Edge equivariance error  (min/med/max):\", ee_s)\n",
        "print(\"Graph invariance error   (min/med/max):\", ge_s)\n",
        "\n",
        "print(\"\\nStress test: INCONSISTENT shuffling (graph invariance should break)\")\n",
        "ne_b, ee_b, ge_b = run_trials(num_trials=100, consistent=False)\n",
        "print(\"Graph invariance error   (min/med/max):\", ge_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SjuXxIC0_c2-",
      "metadata": {
        "id": "SjuXxIC0_c2-"
      },
      "source": [
        "# Conclusions\n",
        "\n",
        "Today, we have learnt how to code a Message Passing GNN (GIN) and applied it to the task of link prediction (predicting whether two nodes are connected). Then, we dived into expressivity (a model's ability to learn and represent complex functions over graph-structured data) and tested it with WL-1. Finally, we explore robustness to input variations of GNNs (an important property, given that graphs are not-ordered) looking into both layer-equivariance (shuffled input at layer level reflect in the layer output accordingly) and model-invariance (shuffled input at model level should not impact predictions)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "grhIvYvvA0kZ",
      "metadata": {
        "id": "grhIvYvvA0kZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
